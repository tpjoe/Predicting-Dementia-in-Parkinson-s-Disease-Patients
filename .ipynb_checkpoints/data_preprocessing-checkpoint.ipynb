{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing functions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy import special\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import updatedBoltzmannclean\n",
    "import pickle\n",
    "import math\n",
    "# import boltzmannclean\n",
    "# from fancyimpute import IterativeImputer as fancyIterativeImputer\n",
    "# import pandas_bokeh\n",
    "# pandas_bokeh.output_notebook()\n",
    "\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import CustomJS, ColumnDataSource, Slider, HoverTool\n",
    "# import holoviews as hv\n",
    "# from holoviews import dim, opts\n",
    "# hv.extension('bokeh', 'matplotlib')\n",
    "# output_notebook()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics import f1_score, average_precision_score, multilabel_confusion_matrix, recall_score, roc_auc_score\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df size is  (1836, 64)\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "df = pd.read_csv('data/dataRaw2.csv', na_values='.') # 2 is the second round of data where APOE count exists\n",
    "print('df size is ', df.shape)\n",
    "\n",
    "# list out colnames\n",
    "colNames = ['summary_id', 'collectiondate_median', 'visit', 'led', \n",
    "            'site', 'agevisit', 'race', 'gender', 'ethnicity', 'education',  #demographic\n",
    "            'handedness', 'Firstdeg_PD', 'Seconddeg_PD', 'veteran_status', #demographic\n",
    "            'cognitive_status', 'updrs3_total_combo', 'modified_hy', #disease/clinical characteristics\n",
    "            'disease_duration_onset', 'gds_total', 'disease_duration_dx', #disease/clinical characteristics \n",
    "            'dx_tremor', 'dx_rigidity', 'dx_bradykinesia', 'dx_instability', 'dx_dominant_side',\n",
    "            'mayo_1_recent', 'mayo_2_recent', 'mayo_3_recent', 'mayo_4_recent', \n",
    "            'mayo_5_recent', 'mayo_6_recent', 'mayo_7_recent', 'mayo_8_3weeks', #sleep questions\n",
    "            'npi_A_total', 'npi_B_total', 'npi_C_total', 'npi_D_total', \n",
    "            'npi_E_total', 'npi_F_total', 'npi_G_total', 'npi_H_total', \n",
    "            'npi_I_total', 'npi_J_total', 'npi_K_total', 'npi_L_total', 'visual_halluc', \n",
    "            'GBA_carrier', 'ApoE', 'LRRK2_carrier',  'MAPT', 'SNCA_rs356219', #genetic\n",
    "            'animals', 'letter_fluency', # Cognitive-verbal fluency\n",
    "            'hvlt_total_recall', 'hvlt_RDI', 'hvlt_trial4_correct', #cognitive - learning/memory\n",
    "            'jolo_total_correct', #cognitive - visuospatial\n",
    "            'wais_digit_symbol_score', 'letter_number_sequencing_total',\n",
    "            'trails_a_seconds_utc150', 'trails_b_seconds_utc300', \n",
    "            'trailsbminusa', #cognitive - attention\n",
    "            'moca_score_unadjusted' #cognitive - global\n",
    "             ]\n",
    "df['ApoE'] = df['ApoE'].apply(lambda x: np.NaN if math.isnan(x) else str(x))\n",
    "multipleCateg = ['race', 'handedness', 'dx_tremor', 'dx_rigidity', 'dx_bradykinesia', 'dx_instability', 'dx_dominant_side', 'SNCA_rs356219', 'ApoE']\n",
    "binaryCateg = ['gender', 'ethnicity', 'Firstdeg_PD', 'Seconddeg_PD', 'GBA_carrier', 'LRRK2_carrier', 'MAPT']\n",
    "categ = multipleCateg + binaryCateg\n",
    "\n",
    "# Remove minor cognitive statuses (<5 cases)\n",
    "df = df[colNames]\n",
    "otherStatus = df.loc[df.cognitive_status.isin(['Unknown', 'Other']), 'summary_id'].values\n",
    "df = df.drop(df[df.summary_id.isin(otherStatus)].index)\n",
    "df = df.reset_index().drop(['index'], axis=1)\n",
    "colNames = df.columns\n",
    "\n",
    "# Remove trails_a and trails_ b because trailsminusa is a subtraction of trails_b_seconds_utc300 by trails_a_seconds_utc150\n",
    "df = df.drop(['trails_a_seconds_utc150', 'trails_b_seconds_utc300'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "led                                326\n",
       "handedness                          14\n",
       "Firstdeg_PD                         57\n",
       "Seconddeg_PD                       106\n",
       "veteran_status                     584\n",
       "updrs3_total_combo                  70\n",
       "modified_hy                          6\n",
       "gds_total                           42\n",
       "dx_tremor                           22\n",
       "dx_rigidity                         34\n",
       "dx_bradykinesia                     27\n",
       "dx_instability                      37\n",
       "dx_dominant_side                    30\n",
       "mayo_1_recent                     1172\n",
       "mayo_2_recent                     1181\n",
       "mayo_3_recent                     1175\n",
       "mayo_4_recent                     1176\n",
       "mayo_5_recent                     1185\n",
       "mayo_6_recent                     1185\n",
       "mayo_7_recent                     1177\n",
       "mayo_8_3weeks                     1180\n",
       "npi_A_total                       1092\n",
       "npi_B_total                       1094\n",
       "npi_C_total                       1094\n",
       "npi_D_total                       1094\n",
       "npi_E_total                       1097\n",
       "npi_F_total                       1095\n",
       "npi_G_total                       1098\n",
       "npi_H_total                       1097\n",
       "npi_I_total                       1096\n",
       "npi_J_total                       1095\n",
       "npi_K_total                       1096\n",
       "npi_L_total                       1097\n",
       "visual_halluc                     1099\n",
       "GBA_carrier                         18\n",
       "ApoE                                10\n",
       "LRRK2_carrier                      130\n",
       "MAPT                               211\n",
       "SNCA_rs356219                      208\n",
       "animals                             83\n",
       "letter_fluency                     136\n",
       "hvlt_total_recall                  115\n",
       "hvlt_RDI                           133\n",
       "hvlt_trial4_correct                125\n",
       "jolo_total_correct                  90\n",
       "wais_digit_symbol_score             96\n",
       "letter_number_sequencing_total     158\n",
       "trailsbminusa                       81\n",
       "moca_score_unadjusted               46\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for NA in each column\n",
    "colNA = df.isna().sum() > 0\n",
    "colNA = [colNA.keys().tolist()[i] for i in np.array(np.where(colNA.tolist()))[0, :]]\n",
    "df.isna().sum()[colNA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing NAs with RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get veteran_status, visual_halluc, npi, and mayo out of the data for now\n",
    "npiMayo = [(i) for i in df.columns.tolist() if 'npi' in i or 'mayo' in i or 'visual_halluc' in i or 'veteran_status' in i]\n",
    "df_vis = df.drop(npiMayo, axis=1).copy()\n",
    "notForImpute = ['summary_id', 'collectiondate_median', 'site', 'cognitive_status', 'visit']\n",
    "df_vis = df_vis.drop(notForImpute, axis=1)\n",
    "\n",
    "# start imputing\n",
    "df_vis = updatedBoltzmannclean.clean(dataframe=df_vis, numerical_columns=list(set(df_vis.columns.to_list()) - set(categ)), \n",
    "                                     categorical_columns=categ, tune_rbm=True)\n",
    "df_vis = pd.concat([df[notForImpute], df_vis], axis=1)\n",
    "\n",
    "# Manage ApoE a bit (into columns of counts of each allele)\n",
    "df_vis['ApoE'] = df_vis['ApoE'].apply(str)\n",
    "ApoE = df_vis.ApoE.str.split(pat='.', expand=True)\n",
    "df_vis['ApoE2'] = ApoE[0].apply(lambda x: 1 if x == '2' else 0) + ApoE[1].apply(lambda x: 1 if x == '2' else 0)\n",
    "df_vis['ApoE3'] = ApoE[0].apply(lambda x: 1 if x == '3' else 0) + ApoE[1].apply(lambda x: 1 if x == '3' else 0)\n",
    "df_vis['ApoE4'] = ApoE[0].apply(lambda x: 1 if x == '4' else 0) + ApoE[1].apply(lambda x: 1 if x == '4' else 0)\n",
    "\n",
    "# Encoding y and remaining category\n",
    "df_vis[['cognitive_status']] = df_vis[['cognitive_status']].apply(preprocessing.LabelEncoder().fit_transform)\n",
    "df_vis[['cognitive_status']] = df_vis.cognitive_status.replace({1: 2, 0: 1, 2: 0}) # 0 = No cognitive impairment, 1 = Cognitive impairment but no dementia, 2 = dementia\n",
    "encodeCol = ['race', 'gender', 'SNCA_rs356219', 'site', 'dx_tremor', 'dx_rigidity', 'dx_bradykinesia', 'dx_instability', 'ApoE']\n",
    "df_vis[encodeCol] = df_vis[encodeCol].astype(str).apply(preprocessing.LabelEncoder().fit_transform)\n",
    "\n",
    "# Add APOE4 coulumn (of having or not having)\n",
    "df_vis['APOE_E4'] = df_vis['ApoE4'].apply(lambda x: 1 if x > 0 else 0)\n",
    "categ = categ + ['APOE_E4']\n",
    "\n",
    "# One of the patient visit number has a typo\n",
    "df_vis.loc[df.summary_id == 'PWA10-0183', 'visit'] = df_vis.loc[df.summary_id == 'PWA10-0183', 'visit'] - 1\n",
    "\n",
    "# #dump df_vis\n",
    "# pickle.dump(df_vis, open(\"data/pickled_df_vis.p\", \"wb\"), protocol=4)   # uncomment if you need to save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating df_vis_byID (grouped by ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_vis\n",
    "df_vis = pickle.load(open(\"data/pickled_df_vis.p\", \"rb\"))\n",
    "\n",
    "# Revised adjustment\n",
    "df_vis.loc[df_vis.summary_id=='PWA13-0570', 'trailsbminusa'] = 95\n",
    "\n",
    "# remove those that have negative disease_duration_dx\n",
    "# df_vis = df_vis.loc[~df_vis.disease_duration_dx.isin([-803, -2]), ]\n",
    "\n",
    "# Create dataframe grouped by id and visit, also adding interval between each visit, and type of disease progression\n",
    "df_vis_byID = df_vis.groupby(['summary_id', 'visit']).mean()\n",
    "df_vis_byID['collectiondate_median'] = pd.to_datetime(df_vis_byID['collectiondate_median'], yearfirst=True, format='%Y%m%d')\n",
    "df_vis_byID['interval'] = np.where(df_vis_byID.index.get_level_values('visit').values != 1, \n",
    "                                   df_vis_byID.collectiondate_median - df_vis_byID.collectiondate_median.shift(), 0)\n",
    "df_vis_byID['interval'] = df_vis_byID['interval'].apply(lambda l: l.days)\n",
    "df_vis_byID['interval(mth)'] = df_vis_byID['interval'].apply(lambda x: round(x/30))\n",
    "df_vis_byID['interval(yr)'] = df_vis_byID['interval'].apply(lambda x: round(x/365))\n",
    "df_vis_byID['days_since_1st_visit'] = df_vis_byID['interval'].groupby(level=[0]).cumsum()\n",
    "df_vis_byID['years_since_1st_visit'] = df_vis_byID['days_since_1st_visit'].apply(lambda x: round(x/365))\n",
    "df_vis_byID['n_visit'] = [(df_vis_byID.loc[ID].index.get_level_values('visit')).max() for ID in df_vis_byID.index.get_level_values('summary_id')]\n",
    "\n",
    "progression = []\n",
    "firstPDYear = []\n",
    "for ID in df_vis_byID.index.get_level_values('summary_id').unique():\n",
    "    status = df_vis_byID.loc[ID].cognitive_status.values\n",
    "    PDYears = df_vis_byID.loc[ID].disease_duration_onset.values\n",
    "    firstPDYear = firstPDYear + [PDYears[0]]*len(PDYears)\n",
    "    if len(np.unique(status)) == 1:\n",
    "        progression = progression + [str(np.unique(status)[0])]*len(status)\n",
    "    elif (status == sorted(status)).all():\n",
    "        # if status is developmental (0 --> 1 --> 2)\n",
    "        progression = progression + [''.join(str(x) for x in np.unique(status))]*len(status)\n",
    "    elif (set([0, 2]).issubset(status)):\n",
    "        if (np.argwhere(status==0)[-1] > np.argwhere(status==2)[0]):\n",
    "            # removing status that is unlikely\n",
    "            progression = progression + ['*20*']*len(status)\n",
    "        else:\n",
    "            progression = progression + ['9999']*len(status)\n",
    "    elif (set([1, 2]).issubset(status)):\n",
    "        if (np.argwhere(status==1)[-1] > np.argwhere(status==2)[0]):\n",
    "            # removing status that is unlikely\n",
    "            progression = progression + ['*21*']*len(status)\n",
    "            continue\n",
    "        else:\n",
    "            progression = progression + ['9999']*len(status)\n",
    "            continue\n",
    "    else:\n",
    "        progression = progression + ['9999']*len(status)\n",
    "\n",
    "df_vis_byID['progression'] = progression\n",
    "df_vis_byID['firstPDYear'] = firstPDYear\n",
    "df_vis_byID['disease_duration_onset_calculated'] = df_vis_byID['firstPDYear'] + df_vis_byID['years_since_1st_visit']\n",
    "df_vis_byID = df_vis_byID.drop(['firstPDYear'], axis=1)\n",
    "\n",
    "oriID = list(df_vis_byID.index.get_level_values('summary_id'))\n",
    "shiftedID = oriID[1:] + ['0']\n",
    "df_vis_byID['next_cognitive_status'] = np.where([oriID[x] == shiftedID[x] for x in range(len(shiftedID))], \n",
    "                                    df_vis_byID.cognitive_status.shift(-1).apply(lambda x: x if np.isnan(x) else str(int(x))), '9999')\n",
    "\n",
    "# binning PD onset\n",
    "bins = np.linspace(0, 44, 45)\n",
    "df_vis_byID['PD_onset_binned'] =  np.digitize(df_vis_byID['disease_duration_onset_calculated'], bins)\n",
    "\n",
    "# Get rid of the *21* or *20*\n",
    "df_vis_byID = df_vis_byID.loc[~(df_vis_byID.progression.isin(['*21*', '*20*'])), :].copy()\n",
    "\n",
    "colName = df_vis_byID.columns.tolist()\n",
    "colName = [colName[0]] + colName[-10:] + colName[1:-10]\n",
    "df_vis_byID = df_vis_byID[colName]\n",
    "featureColumns  = colName[13:] # excluding all these progression, intervals, and cognitive_status\n",
    "\n",
    "# saving\n",
    "pickle.dump(df_vis_byID, open('data/pickled_df_vis_byID.p', 'wb'))\n",
    "pickle.dump(featureColumns, open('data/pickled_featureColumns.p', 'wb'))\n",
    "pickle.dump(multipleCateg, open('data/pickled_multipleCateg.p', 'wb'))\n",
    "pickle.dump(categ, open('data/pickled_categ.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to MATLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To matlab\n",
    "df_vis_byID = pickle.load(open('data/pickled_df_vis_byID.p', 'rb'))\n",
    "featureColumns = pickle.load(open('data/pickled_featureColumns.p', 'rb'))\n",
    "multipleCateg = pickle.load(open('data/pickled_multipleCateg.p', 'rb'))\n",
    "categ = pickle.load(open('data/pickled_categ.p', 'rb'))\n",
    "\n",
    "featureColumnsMATLAB = [i for i in featureColumns if not(i in ['ApoE', 'ApoE2', 'ApoE3', 'ApoE4', 'SNCA_rs356219', 'LRRK2_carrier', 'disease_duration_onset'])]\n",
    "multipleCategMATLAB = [i for i in multipleCateg if not(i in ['ApoE', 'SNCA_rs356219'])]\n",
    "\n",
    "Xs = dict()\n",
    "ys = dict()\n",
    "scaling = preprocessing.StandardScaler()  #MinMaxScaler #StandardScaler\n",
    "\n",
    "df_vis_byYear = df_vis_byID.reset_index().groupby(['summary_id', 'years_since_1st_visit']).mean()\n",
    "X = df_vis_byYear[featureColumnsMATLAB]\n",
    "y = df_vis_byYear['cognitive_status']\n",
    "dummies = pd.get_dummies(X[multipleCategMATLAB], columns=X[multipleCategMATLAB].columns)\n",
    "X = X.drop(multipleCategMATLAB, axis=1)\n",
    "X = pd.concat([X, dummies], axis=1)\n",
    "\n",
    "X = X.rename(columns={'handedness_1.0':'handedness_1', 'handedness_2.0':'handedness_2', 'handedness_3.0':'handedness_3',\n",
    "                  'dx_dominant_side_1.0':'dx_dominant_side_1', 'dx_dominant_side_2.0':'dx_dominant_side_2',\n",
    "                  'dx_dominant_side_3.0':'dx_dominant_side_3'})\n",
    "X[X.columns[~X.columns.isin(categ)]] = scaling.fit_transform(X[X.columns[~X.columns.isin(categ)]])\n",
    "# X[X.columns[~X.columns.isin(categ)]] = X[X.columns[~X.columns.isin(categ)]].apply(lambda x: x + abs(min(x)))\n",
    "\n",
    "for i in range(0, 6):\n",
    "    ys[i] = y.loc[(y.index.get_level_values('years_since_1st_visit')==i), ]\n",
    "    ys_index = ys[i].index.get_level_values('summary_id')\n",
    "    Xs[i] = X.loc[(X.index.get_level_values('years_since_1st_visit')==0) & (X.index.get_level_values('summary_id').isin(ys_index)), ]\n",
    "    \n",
    "for i, x in enumerate(Xs.keys()):\n",
    "    Xs[x].reset_index(drop=True).to_csv('data/X_yearsince1stvisit_stdScaled.{}.csv'.format(i))\n",
    "    ys[x].reset_index(drop=True).to_csv('data/y_yearsince1stvisit_stdScaled.{}.csv'.format(i), header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1795.000000\n",
       "mean        7.611699\n",
       "std         6.150036\n",
       "min         0.000000\n",
       "25%         3.000000\n",
       "50%         6.000000\n",
       "75%        11.000000\n",
       "max        41.000000\n",
       "Name: disease_duration_dx, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vis_byID.disease_duration_dx.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_vis_byID = pickle.load(open('data/pickled_df_vis_byID.p', 'rb'))\n",
    "# featureColumns = pickle.load(open('data/pickled_featureColumns.p', 'rb'))\n",
    "# multipleCateg = pickle.load(open('data/pickled_multipleCateg.p', 'rb'))\n",
    "# categ = pickle.load(open('data/pickled_categ.p', 'rb'))\n",
    "\n",
    "df_vis_byID.to_csv('data/csv_df_vis_by_ID.csv')\n",
    "pd.Series(featureColumns).to_csv('data/csv_featureColumns.csv', header=False)\n",
    "pd.Series(multipleCateg).to_csv('data/csv_multipleCateg.csv', header=False)\n",
    "pd.Series(categ).to_csv('data/csv_categ.csv', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
